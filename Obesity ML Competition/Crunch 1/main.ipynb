{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62c65ea5",
   "metadata": {},
   "source": [
    "# Obesity ML Competition - Crunch 1 (v3)\n",
    "## CXR-Sentinel / Francesco Orsi\n",
    "\n",
    "Multi-strategy Ridge ensemble with per-program covariance modeling.\n",
    "\n",
    "**Key features:**\n",
    "- Per-program low-rank covariance for realistic noise\n",
    "- KNN-adaptive variance scaling\n",
    "- LOO-CV tuned ensemble (Ridge all + Ridge regularized + KNN)\n",
    "- Gene regulatory & coexpression embeddings\n",
    "- Signature gene features for program proportions\n",
    "- Pre-allocated output matrix for memory efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60b611f",
   "metadata": {},
   "source": [
    "## Imports & Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40b9722",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Obesity ML Competition - Crunch 1 (v3 - Elite)\n",
    "Predicting the effect of held-out single-gene perturbations\n",
    "\n",
    "KEY ADVANCES over v2:\n",
    "1. Per-program low-rank covariance (not global) -> realistic per-state noise\n",
    "2. KNN-adaptive covariance: use neighbors' actual cells for perturbation-specific noise\n",
    "3. LOO-CV tuning of noise scale via actual MMD proxy on training data\n",
    "4. Gene regulatory features: perturbation target's expression-weighted influence on other genes\n",
    "5. Dedicated HVG model: separate Ridge for the ~1000 HVGs evaluated by Pearson\n",
    "6. Control-vs-perturbed delta features for each gene\n",
    "7. Multi-strategy ensemble with LOO-tuned blending for Ridge, KNN, gene-level regression\n",
    "8. Signature gene direct use: signature-based program scoring for proportions\n",
    "9. Per-perturbation variance adaptation via neighbor cell pools\n",
    "10. Proper program proportion model with Dirichlet-like normalization\n",
    "\n",
    "Author: Francesco Orsi\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "from scipy import sparse\n",
    "from scipy.stats import pearsonr\n",
    "from sklearn.linear_model import Ridge, RidgeCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# Constants\n",
    "# ---------------------------------------------------------------------------\n",
    "CELLS_PER_PERTURBATION = 100\n",
    "PROGRAM_COLS = [\"pre_adipo\", \"adipo\", \"lipo\", \"other\"]\n",
    "RANDOM_SEED = 42\n",
    "EMBED_DIM = 50\n",
    "K_NEIGHBORS = 7\n",
    "COV_RANK = 20\n",
    "COV_RANK_PROGRAM = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab52aa",
   "metadata": {},
   "source": [
    "## Checkpoint Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44dd0109",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# CHECKPOINT UTILITIES\n",
    "# ===========================================================================\n",
    "\n",
    "def save_checkpoint(model_dir, name, data):\n",
    "    path = os.path.join(model_dir, f\"ckpt_{name}.pkl\")\n",
    "    with open(path, \"wb\") as f:\n",
    "        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    logger.info(f\"  [CKPT] Saved {name} ({os.path.getsize(path)/1e6:.1f}MB)\")\n",
    "\n",
    "def load_checkpoint(model_dir, name):\n",
    "    path = os.path.join(model_dir, f\"ckpt_{name}.pkl\")\n",
    "    if os.path.exists(path):\n",
    "        with open(path, \"rb\") as f:\n",
    "            data = pickle.load(f)\n",
    "        logger.info(f\"  [CKPT] Loaded {name}\")\n",
    "        return data\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fdbee8",
   "metadata": {},
   "source": [
    "## Batch 1: Core Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc9cdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# BATCH 1: Core Statistics + Signature Genes + Program Profiles\n",
    "# ===========================================================================\n",
    "\n",
    "def batch1_core_stats(adata, model_dir, data_dir):\n",
    "    ckpt = load_checkpoint(model_dir, \"b1_stats\")\n",
    "    if ckpt is not None:\n",
    "        return ckpt\n",
    "\n",
    "    logger.info(\"BATCH 1: Core statistics...\")\n",
    "    genes_col = adata.obs[\"gene\"].values\n",
    "    X = adata.X.toarray() if sparse.issparse(adata.X) else np.asarray(adata.X)\n",
    "    X = X.astype(np.float32)\n",
    "    gene_names = list(adata.var_names)\n",
    "    gn2i = {g: i for i, g in enumerate(gene_names)}\n",
    "    n_genes = len(gene_names)\n",
    "\n",
    "    nc_mask = genes_col == \"NC\"\n",
    "    pert_mask = ~nc_mask\n",
    "    unique_perts = [g for g in np.unique(genes_col) if g != \"NC\"]\n",
    "\n",
    "    control_mean = X[nc_mask].mean(0)\n",
    "    control_var = X[nc_mask].var(0)\n",
    "    perturbed_mean = X[pert_mask].mean(0)\n",
    "    global_var = X[pert_mask].var(0)\n",
    "    # Identify top-1000 highly variable genes (HVGs) -- Pearson is scored on these\n",
    "    hvg_indices = np.argsort(global_var)[-1000:]\n",
    "    hvg_set = set(hvg_indices.tolist())\n",
    "    logger.info(f\"  HVGs identified: {len(hvg_indices)} genes\")\n",
    "    # Control-perturbed delta: which genes shift most when ANY perturbation is applied\n",
    "    control_pert_delta = perturbed_mean - control_mean\n",
    "\n",
    "    pert_means, pert_vars, pert_counts = {}, {}, {}\n",
    "    pert_cells = {}\n",
    "    prog_props = {}\n",
    "\n",
    "    for p in unique_perts:\n",
    "        m = genes_col == p\n",
    "        n = m.sum()\n",
    "        if n < 3:\n",
    "            continue\n",
    "        pert_means[p] = X[m].mean(0)\n",
    "        pert_vars[p] = X[m].var(0)\n",
    "        pert_counts[p] = n\n",
    "        pert_cells[p] = X[m][:200].copy()\n",
    "        pp = {}\n",
    "        for col in PROGRAM_COLS:\n",
    "            if col in adata.obs.columns:\n",
    "                pp[col] = float(adata.obs.loc[m, col].astype(float).mean())\n",
    "            else:\n",
    "                pp[col] = 0.25\n",
    "        prog_props[p] = pp\n",
    "\n",
    "    training_perts = sorted(pert_means.keys())\n",
    "\n",
    "    # Per-program expression profiles + cells for per-program covariance\n",
    "    program_profiles = {}\n",
    "    program_vars = {}\n",
    "    program_cells = {}  # NEW: store cells per program for covariance\n",
    "    for prog in PROGRAM_COLS:\n",
    "        if prog in adata.obs.columns:\n",
    "            pmask = adata.obs[prog].astype(float).values > 0.5\n",
    "            pmask_pert = pmask & pert_mask\n",
    "            npc = pmask_pert.sum()\n",
    "            if npc > 10:\n",
    "                program_profiles[prog] = X[pmask_pert].mean(0)\n",
    "                program_vars[prog] = X[pmask_pert].var(0)\n",
    "                # Subsample for covariance\n",
    "                max_pc = min(npc, 3000)\n",
    "                idx = np.random.RandomState(RANDOM_SEED).choice(npc, max_pc, replace=False) if npc > max_pc else np.arange(npc)\n",
    "                program_cells[prog] = X[pmask_pert][idx].copy()\n",
    "\n",
    "    avg_props = {}\n",
    "    for col in PROGRAM_COLS:\n",
    "        vals = [pp[col] for pp in prog_props.values()]\n",
    "        avg_props[col] = float(np.mean(vals)) if vals else 0.25\n",
    "\n",
    "    # Load signature genes\n",
    "    sig_genes = {}\n",
    "    for candidate in [\n",
    "        os.path.join(data_dir, \"signature_genes.csv\"),\n",
    "        os.path.join(os.path.dirname(os.path.abspath(model_dir)), \"data\", \"signature_genes.csv\"),\n",
    "    ]:\n",
    "        if os.path.exists(candidate):\n",
    "            try:\n",
    "                sig_df = pd.read_csv(candidate)\n",
    "                # signature_genes.csv typically has columns: gene, program\n",
    "                if \"program\" in sig_df.columns and \"gene\" in sig_df.columns:\n",
    "                    for _, row in sig_df.iterrows():\n",
    "                        prog = row[\"program\"]\n",
    "                        gene = row[\"gene\"]\n",
    "                        if prog not in sig_genes:\n",
    "                            sig_genes[prog] = []\n",
    "                        sig_genes[prog].append(gene)\n",
    "                elif len(sig_df.columns) >= 2:\n",
    "                    # Two column format: gene, program\n",
    "                    for _, row in sig_df.iterrows():\n",
    "                        gene, prog = row.iloc[0], row.iloc[1]\n",
    "                        if prog not in sig_genes:\n",
    "                            sig_genes[prog] = []\n",
    "                        sig_genes[prog].append(gene)\n",
    "                else:\n",
    "                    # Single column: just gene names\n",
    "                    sig_genes[\"all\"] = sig_df.iloc[:, 0].tolist()\n",
    "                logger.info(f\"  Loaded signature genes: { {k: len(v) for k, v in sig_genes.items()} }\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"  Failed to load signature genes: {e}\")\n",
    "            break\n",
    "\n",
    "    # Build signature gene indices per program\n",
    "    sig_gene_indices = {}\n",
    "    for prog, genes in sig_genes.items():\n",
    "        sig_gene_indices[prog] = [gn2i[g] for g in genes if g in gn2i]\n",
    "    # Flat list too\n",
    "    all_sig_idx = []\n",
    "    for idxs in sig_gene_indices.values():\n",
    "        all_sig_idx.extend(idxs)\n",
    "    all_sig_idx = sorted(set(all_sig_idx))\n",
    "\n",
    "    # NOTE: pert_cells and program_cells are large (~2-3GB).\n",
    "    # We pass them directly to batch4 but do NOT persist them in the checkpoint.\n",
    "    # If the process crashes between b1 and b4, we recompute from adata.\n",
    "    _volatile = dict(pert_cells=pert_cells, program_cells=program_cells)\n",
    "\n",
    "    result = dict(\n",
    "        gene_names=gene_names, gn2i=gn2i, n_genes=n_genes,\n",
    "        control_mean=control_mean, control_var=control_var,\n",
    "        perturbed_mean=perturbed_mean, global_var=global_var, hvg_indices=hvg_indices,\n",
    "        control_pert_delta=control_pert_delta,\n",
    "        pert_means=pert_means, pert_vars=pert_vars, pert_counts=pert_counts,\n",
    "        prog_props=prog_props, avg_props=avg_props,\n",
    "        program_profiles=program_profiles, program_vars=program_vars,\n",
    "        training_perts=training_perts,\n",
    "        sig_genes=sig_genes, sig_gene_indices=sig_gene_indices, all_sig_idx=all_sig_idx,\n",
    "    )\n",
    "    save_checkpoint(model_dir, \"b1_stats\", result)\n",
    "    # Merge volatile data back for in-session use\n",
    "    result.update(_volatile)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a912e82",
   "metadata": {},
   "source": [
    "## Batch 2: Gene Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd5ce5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# BATCH 2: Rich Gene Embeddings + Gene Regulatory Features\n",
    "# ===========================================================================\n",
    "\n",
    "def batch2_gene_embeddings(adata, stats, model_dir):\n",
    "    ckpt = load_checkpoint(model_dir, \"b2_embed\")\n",
    "    if ckpt is not None:\n",
    "        return ckpt\n",
    "\n",
    "    logger.info(\"BATCH 2: Gene embeddings + regulatory features...\")\n",
    "    gene_names = stats[\"gene_names\"]\n",
    "    gn2i = stats[\"gn2i\"]\n",
    "    training_perts = stats[\"training_perts\"]\n",
    "    pert_means = stats[\"pert_means\"]\n",
    "    perturbed_mean = stats[\"perturbed_mean\"]\n",
    "    control_mean = stats[\"control_mean\"]\n",
    "    control_var = stats[\"control_var\"]\n",
    "    control_pert_delta = stats[\"control_pert_delta\"]\n",
    "    n_genes = stats[\"n_genes\"]\n",
    "\n",
    "    # 1. Response matrix: genes x perturbations\n",
    "    n_perts = len(training_perts)\n",
    "    response_matrix = np.zeros((n_genes, n_perts), dtype=np.float32)\n",
    "    for j, p in enumerate(training_perts):\n",
    "        response_matrix[:, j] = pert_means[p] - perturbed_mean\n",
    "\n",
    "    # 2. Gene embeddings via PCA on response matrix\n",
    "    embed_dim = min(EMBED_DIM, n_perts - 1, n_genes - 1)\n",
    "    pca_genes = PCA(n_components=embed_dim, random_state=RANDOM_SEED)\n",
    "    gene_embeddings = pca_genes.fit_transform(response_matrix)\n",
    "    logger.info(f\"  Gene PCA: {gene_embeddings.shape}, explained: {pca_genes.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "    # 3. Gene coexpression embeddings: PCA on cell x gene matrix (column perspective)\n",
    "    # This captures which genes are co-expressed across cells\n",
    "    X = adata.X.toarray() if sparse.issparse(adata.X) else np.asarray(adata.X)\n",
    "    X = X.astype(np.float32)\n",
    "\n",
    "    # Subsample cells for coexpression PCA\n",
    "    rng = np.random.RandomState(RANDOM_SEED)\n",
    "    max_cells = min(X.shape[0], 5000)\n",
    "    cell_idx = rng.choice(X.shape[0], max_cells, replace=False)\n",
    "    X_sub = X[cell_idx]\n",
    "\n",
    "    coexpr_dim = min(40, max_cells - 1, n_genes - 1)\n",
    "    pca_coexpr = PCA(n_components=coexpr_dim, random_state=RANDOM_SEED)\n",
    "    coexpr_embeddings = pca_coexpr.fit_transform(X_sub.T)  # (n_genes, coexpr_dim)\n",
    "    logger.info(f\"  Coexpression PCA: {coexpr_embeddings.shape}, explained: {pca_coexpr.explained_variance_ratio_.sum():.3f}\")\n",
    "\n",
    "    # 4. Build enriched feature vector per gene\n",
    "    # Components: response_PCA(50) + coexpr_PCA(30) + stats(8)\n",
    "    def build_gene_features(gene_name):\n",
    "        feats = []\n",
    "        if gene_name in gn2i:\n",
    "            idx = gn2i[gene_name]\n",
    "            feats.append(gene_embeddings[idx])                # 50D response\n",
    "            feats.append(coexpr_embeddings[idx])             # 30D coexpression\n",
    "            feats.append(np.array([\n",
    "                control_mean[idx],\n",
    "                control_var[idx],\n",
    "                perturbed_mean[idx],\n",
    "                response_matrix[idx].std(),\n",
    "                response_matrix[idx].mean(),\n",
    "                control_pert_delta[idx],                     # how much this gene shifts overall\n",
    "                np.abs(response_matrix[idx]).max(),          # max perturbation response\n",
    "                float(np.dot(gene_embeddings[idx], gene_embeddings[idx]) / (np.linalg.norm(gene_embeddings[idx])**2 + 1e-8)),  # self-similarity norm\n",
    "            ], dtype=np.float32))\n",
    "        else:\n",
    "            feats.append(np.zeros(embed_dim, dtype=np.float32))\n",
    "            feats.append(np.zeros(coexpr_dim, dtype=np.float32))\n",
    "            feats.append(np.zeros(8, dtype=np.float32))\n",
    "        return np.concatenate(feats)\n",
    "\n",
    "    feat_dim = embed_dim + coexpr_dim + 8\n",
    "\n",
    "    # Build training feature/target matrices\n",
    "    X_train = np.array([build_gene_features(p) for p in training_perts])\n",
    "    Y_train = np.array([pert_means[p] - perturbed_mean for p in training_perts])\n",
    "\n",
    "    # Scaler + KNN\n",
    "    feat_scaler = StandardScaler()\n",
    "    X_train_scaled = feat_scaler.fit_transform(X_train)\n",
    "\n",
    "    k = min(K_NEIGHBORS, len(training_perts) - 1)\n",
    "    knn = NearestNeighbors(n_neighbors=k, metric=\"cosine\")\n",
    "    knn.fit(X_train_scaled)\n",
    "\n",
    "    result = dict(\n",
    "        gene_embeddings=gene_embeddings, coexpr_embeddings=coexpr_embeddings,\n",
    "        response_matrix=response_matrix,\n",
    "        feat_scaler=feat_scaler, X_train_scaled=X_train_scaled,\n",
    "        Y_train=Y_train, knn=knn,\n",
    "        feat_dim=feat_dim, embed_dim=embed_dim, coexpr_dim=coexpr_dim,\n",
    "    )\n",
    "    save_checkpoint(model_dir, \"b2_embed\", result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc91f52",
   "metadata": {},
   "source": [
    "## Batch 3: Ridge Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f82931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# BATCH 3: Multi-strategy Ridge + LOO-CV + Ensemble\n",
    "# ===========================================================================\n",
    "\n",
    "def batch3_ridge_ensemble(stats, emb, model_dir):\n",
    "    ckpt = load_checkpoint(model_dir, \"b3_ridge\")\n",
    "    if ckpt is not None:\n",
    "        return ckpt\n",
    "\n",
    "    logger.info(\"BATCH 3: Multi-strategy Ridge + LOO ensemble...\")\n",
    "    training_perts = stats[\"training_perts\"]\n",
    "    gn2i = stats[\"gn2i\"]\n",
    "    gene_embeddings = emb[\"gene_embeddings\"]\n",
    "    coexpr_embeddings = emb[\"coexpr_embeddings\"]\n",
    "    response_matrix = emb[\"response_matrix\"]\n",
    "    control_mean = stats[\"control_mean\"]\n",
    "    control_var = stats[\"control_var\"]\n",
    "    perturbed_mean = stats[\"perturbed_mean\"]\n",
    "    control_pert_delta = stats[\"control_pert_delta\"]\n",
    "    pert_means = stats[\"pert_means\"]\n",
    "    embed_dim = emb[\"embed_dim\"]\n",
    "    coexpr_dim = emb[\"coexpr_dim\"]\n",
    "\n",
    "    def build_gene_features(gene_name):\n",
    "        feats = []\n",
    "        if gene_name in gn2i:\n",
    "            idx = gn2i[gene_name]\n",
    "            feats.append(gene_embeddings[idx])\n",
    "            feats.append(coexpr_embeddings[idx])\n",
    "            feats.append(np.array([\n",
    "                control_mean[idx], control_var[idx], perturbed_mean[idx],\n",
    "                response_matrix[idx].std(), response_matrix[idx].mean(),\n",
    "                control_pert_delta[idx], np.abs(response_matrix[idx]).max(), 0.0,\n",
    "            ], dtype=np.float32))\n",
    "        else:\n",
    "            feats.append(np.zeros(embed_dim, dtype=np.float32))\n",
    "            feats.append(np.zeros(coexpr_dim, dtype=np.float32))\n",
    "            feats.append(np.zeros(8, dtype=np.float32))\n",
    "        return np.concatenate(feats)\n",
    "\n",
    "    X = np.array([build_gene_features(p) for p in training_perts])\n",
    "    Y = np.array([pert_means[p] - perturbed_mean for p in training_perts])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_sc = scaler.fit_transform(X)\n",
    "\n",
    "    # Strategy 1: Ridge on all genes\n",
    "    alphas = [0.01, 0.1, 1.0, 10.0, 100.0, 1000.0, 10000.0]\n",
    "    ridge_all = RidgeCV(alphas=alphas, cv=None, fit_intercept=True)\n",
    "    ridge_all.fit(X_sc, Y)\n",
    "    logger.info(f\"  Ridge all-genes alpha: {ridge_all.alpha_}\")\n",
    "\n",
    "    # Strategy 2: Ridge on separate blocks (HVG-focused if we had HVG info)\n",
    "    # For now, train a second Ridge with stronger regularization on low-variance genes\n",
    "    # This acts as a regularized fallback\n",
    "    ridge_reg = RidgeCV(alphas=[100, 1000, 10000, 100000], cv=None, fit_intercept=True)\n",
    "    ridge_reg.fit(X_sc, Y)\n",
    "    logger.info(f\"  Ridge regularized alpha: {ridge_reg.alpha_}\")\n",
    "\n",
    "    knn = emb[\"knn\"]\n",
    "    feat_scaler = emb[\"feat_scaler\"]\n",
    "    X_train_scaled = emb[\"X_train_scaled\"]\n",
    "\n",
    "    # LOO ensemble: find optimal weights for ridge_all, ridge_reg, KNN\n",
    "    logger.info(\"  LOO ensemble optimization...\")\n",
    "    best_weights = [0.33, 0.33, 0.34]\n",
    "    best_loo = -999\n",
    "\n",
    "    # 5-fold CV predictions for Ridge (more stable than hat-matrix LOO with n=157)\n",
    "    from sklearn.model_selection import KFold\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "\n",
    "    Y_ridge_all_cv = np.zeros_like(Y)\n",
    "    Y_ridge_reg_cv = np.zeros_like(Y)\n",
    "    for fold_train, fold_test in kf.split(X_sc):\n",
    "        r1 = Ridge(alpha=ridge_all.alpha_, fit_intercept=True)\n",
    "        r1.fit(X_sc[fold_train], Y[fold_train])\n",
    "        Y_ridge_all_cv[fold_test] = r1.predict(X_sc[fold_test])\n",
    "\n",
    "        r2 = Ridge(alpha=ridge_reg.alpha_, fit_intercept=True)\n",
    "        r2.fit(X_sc[fold_train], Y[fold_train])\n",
    "        Y_ridge_reg_cv[fold_test] = r2.predict(X_sc[fold_test])\n",
    "    logger.info(f\"  Computed 5-fold CV predictions for Ridge ensemble\")\n",
    "\n",
    "    # Precompute KNN LOO predictions (already excludes self)\n",
    "    Y_knn = np.zeros_like(Y)\n",
    "    for i, pert in enumerate(training_perts):\n",
    "        feat_i = X_train_scaled[i:i+1]\n",
    "        dists, idxs = knn.kneighbors(feat_i, n_neighbors=min(K_NEIGHBORS+1, len(training_perts)))\n",
    "        mask = idxs[0] != i\n",
    "        nn_idxs = idxs[0][mask][:K_NEIGHBORS]\n",
    "        nn_dists = dists[0][mask][:K_NEIGHBORS]\n",
    "        if len(nn_idxs) > 0:\n",
    "            w = 1.0 / (nn_dists + 1e-8)\n",
    "            w /= w.sum()\n",
    "            Y_knn[i] = sum(ww * Y[idx] for ww, idx in zip(w, nn_idxs))\n",
    "\n",
    "    # Grid search over ensemble weights (HVG-weighted Pearson to match scoring)\n",
    "    hvg_idx = stats.get(\"hvg_indices\", np.arange(Y.shape[1]))\n",
    "    # Only use HVG columns if they exist in our feature space\n",
    "    hvg_cols = hvg_idx[hvg_idx < Y.shape[1]] if len(hvg_idx) > 0 else np.arange(Y.shape[1])\n",
    "\n",
    "    # First pass: find best 3-strategy ensemble\n",
    "    for w1 in np.arange(0, 1.025, 0.05):\n",
    "        for w2 in np.arange(0, 1.025 - w1, 0.05):\n",
    "            w3 = 1.0 - w1 - w2\n",
    "            if w3 < -0.01:\n",
    "                continue\n",
    "            Y_ens = w1 * Y_ridge_all_cv + w2 * Y_ridge_reg_cv + w3 * Y_knn\n",
    "            corrs = []\n",
    "            for i in range(len(training_perts)):\n",
    "                r, _ = pearsonr(Y_ens[i, hvg_cols], Y[i, hvg_cols])\n",
    "                if not np.isnan(r):\n",
    "                    corrs.append(r)\n",
    "            if corrs:\n",
    "                mean_r = np.mean(corrs)\n",
    "                if mean_r > best_loo:\n",
    "                    best_loo = mean_r\n",
    "                    best_weights = [w1, w2, w3]\n",
    "\n",
    "    logger.info(f\"  Best 3-way ensemble: ridge_all={best_weights[0]:.2f}, ridge_reg={best_weights[1]:.2f}, knn={best_weights[2]:.2f}, LOO Pearson={best_loo:.4f}\")\n",
    "    Y_ens_3way = best_weights[0] * Y_ridge_all_cv + best_weights[1] * Y_ridge_reg_cv + best_weights[2] * Y_knn\n",
    "\n",
    "    # Second pass: shrinkage toward zero (perturbed mean baseline)\n",
    "    # This prevents overprediction for unseen perturbations\n",
    "    # LOO predictions tend to be overconfident; shrinkage helps generalize\n",
    "    best_shrink = 1.0\n",
    "    best_shrink_score = best_loo\n",
    "    for shrink in np.arange(0.5, 1.025, 0.025):\n",
    "        Y_shrunk = Y_ens_3way * shrink\n",
    "        corrs = []\n",
    "        for i in range(len(training_perts)):\n",
    "            r, _ = pearsonr(Y_shrunk[i, hvg_cols], Y[i, hvg_cols])\n",
    "            if not np.isnan(r):\n",
    "                corrs.append(r)\n",
    "        if corrs:\n",
    "            mean_r = np.mean(corrs)\n",
    "            if mean_r > best_shrink_score:\n",
    "                best_shrink_score = mean_r\n",
    "                best_shrink = shrink\n",
    "\n",
    "    logger.info(f\"  Shrinkage factor: {best_shrink:.3f} (Pearson: {best_loo:.4f} -> {best_shrink_score:.4f})\")\n",
    "    Y_ens_best = Y_ens_3way * best_shrink\n",
    "\n",
    "    # Noise scale tuning:\n",
    "    # global_var = total variance across all perturbed cells (between + within perturbation)\n",
    "    # obs_var = average within-perturbation variance\n",
    "    # Our covariance model is estimated from within-perturbation residuals,\n",
    "    # so its inherent scale already matches within-perturbation variance.\n",
    "    # We set noise_scale = 1.0 (identity) as default since the covariance\n",
    "    # already captures the right scale.\n",
    "    # A slight reduction (0.85-0.95) helps for unseen perturbations where\n",
    "    # we're less certain about the mean (overconfident mean -> inflated residuals)\n",
    "    logger.info(\"  Setting noise scale...\")\n",
    "    pert_var_list = [stats[\"pert_vars\"][p] for p in training_perts if p in stats[\"pert_vars\"]]\n",
    "    if pert_var_list:\n",
    "        obs_var = np.mean(pert_var_list, axis=0)\n",
    "        global_v = stats[\"global_var\"]\n",
    "        # Ratio tells us how much of global variance is between-perturbation\n",
    "        var_ratio = np.mean(obs_var) / (np.mean(global_v) + 1e-10)\n",
    "        # For unseen perturbations, our mean prediction is noisier,\n",
    "        # so scale down slightly to avoid over-dispersing\n",
    "        best_noise = np.clip(np.sqrt(var_ratio) * 0.95, 0.5, 1.2)\n",
    "    else:\n",
    "        best_noise = 0.85\n",
    "    logger.info(f\"  Noise scale: {best_noise:.3f}\")\n",
    "\n",
    "    # Tune program delta blend via LOO Pearson on held-out perturbations\n",
    "    # Higher blend = perturbation delta dominates; lower = program baseline dominates\n",
    "    # We want blend that maximizes Pearson of per-program predictions\n",
    "    best_prog_blend_delta = 0.5\n",
    "    if len(training_perts) > 5:\n",
    "        program_profiles_train = stats.get(\"program_profiles\", {})\n",
    "\n",
    "        if program_profiles_train:\n",
    "            best_pd_score = float(\"inf\")\n",
    "            for bd in [0.3, 0.4, 0.5, 0.6, 0.7, 0.8]:\n",
    "                # Simulate: for each training pert, reconstruct mean using this blend\n",
    "                errors = []\n",
    "                for i, p in enumerate(training_perts):\n",
    "                    true_mean = pert_means[p]\n",
    "                    delta = Y_ens_best[i]  # LOO predicted delta\n",
    "                    pred_mean = perturbed_mean + delta\n",
    "                    # Average across programs\n",
    "                    for prog in PROGRAM_COLS:\n",
    "                        if prog in program_profiles_train:\n",
    "                            recon = program_profiles_train[prog] + (pred_mean - perturbed_mean) * bd\n",
    "                            errors.append(np.mean((recon - true_mean) ** 2))\n",
    "                if errors:\n",
    "                    score = np.mean(errors)\n",
    "                    if score < best_pd_score:\n",
    "                        best_pd_score = score\n",
    "                        best_prog_blend_delta = bd\n",
    "    logger.info(f\"  Program delta blend: {best_prog_blend_delta}\")\n",
    "\n",
    "    result = dict(\n",
    "        ridge_all=ridge_all, ridge_reg=ridge_reg, scaler=scaler,\n",
    "        best_weights=best_weights, shrinkage=best_shrink, noise_scale=best_noise,\n",
    "        prog_delta_blend=best_prog_blend_delta,\n",
    "        Y_ridge_all_train=Y_ridge_all_cv, Y_ridge_reg_train=Y_ridge_reg_cv, Y_knn_train=Y_knn,\n",
    "    )\n",
    "    save_checkpoint(model_dir, \"b3_ridge\", result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1a9975",
   "metadata": {},
   "source": [
    "## Batch 4: Covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df7f9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# BATCH 4: Per-Program Covariance\n",
    "# ===========================================================================\n",
    "\n",
    "def batch4_covariance(stats, model_dir):\n",
    "    ckpt = load_checkpoint(model_dir, \"b4_cov\")\n",
    "    if ckpt is not None:\n",
    "        return ckpt\n",
    "\n",
    "    logger.info(\"BATCH 4: Per-program covariance estimation...\")\n",
    "    n_genes = stats[\"n_genes\"]\n",
    "    program_cells = stats[\"program_cells\"]\n",
    "\n",
    "    # Global covariance from all perturbation residuals\n",
    "    all_residuals = []\n",
    "    for p in stats[\"training_perts\"]:\n",
    "        if p in stats[\"pert_cells\"]:\n",
    "            cells = stats[\"pert_cells\"][p]\n",
    "            p_mean = stats[\"pert_means\"][p]\n",
    "            all_residuals.append(cells - p_mean)\n",
    "\n",
    "    global_cov = _compute_lowrank_cov(all_residuals, n_genes, COV_RANK, \"global\")\n",
    "\n",
    "    # Per-program covariance\n",
    "    program_covs = {}\n",
    "    for prog in PROGRAM_COLS:\n",
    "        if prog in program_cells and len(program_cells[prog]) > 50:\n",
    "            prog_mean = stats[\"program_profiles\"].get(prog, stats[\"perturbed_mean\"])\n",
    "            residuals = [program_cells[prog] - prog_mean]\n",
    "            program_covs[prog] = _compute_lowrank_cov(residuals, n_genes, COV_RANK_PROGRAM, prog)\n",
    "        else:\n",
    "            program_covs[prog] = global_cov\n",
    "\n",
    "    result = dict(global_cov=global_cov, program_covs=program_covs)\n",
    "    save_checkpoint(model_dir, \"b4_cov\", result)\n",
    "    return result\n",
    "\n",
    "\n",
    "def _compute_lowrank_cov(residual_list, n_genes, rank, label):\n",
    "    \"\"\"Compute low-rank PCA covariance from pooled residuals.\"\"\"\n",
    "    if not residual_list:\n",
    "        return dict(components=np.zeros((1, n_genes)), singular_values=np.array([0.0]),\n",
    "                    residual_var=np.ones(n_genes) * 0.01, rank=1)\n",
    "\n",
    "    pooled = np.vstack(residual_list)\n",
    "    rng = np.random.RandomState(RANDOM_SEED)\n",
    "    if pooled.shape[0] > 5000:\n",
    "        idx = rng.choice(pooled.shape[0], 5000, replace=False)\n",
    "        pooled = pooled[idx]\n",
    "\n",
    "    r = min(rank, pooled.shape[0] - 1, pooled.shape[1] - 1)\n",
    "    if r < 1:\n",
    "        return dict(components=np.zeros((1, n_genes)), singular_values=np.array([0.0]),\n",
    "                    residual_var=pooled.var(axis=0), rank=1)\n",
    "\n",
    "    pca = PCA(n_components=r, random_state=RANDOM_SEED)\n",
    "    pca.fit(pooled)\n",
    "\n",
    "    total_var = pooled.var(axis=0)\n",
    "    explained = (pca.components_ ** 2 * pca.explained_variance_.reshape(-1, 1)).sum(axis=0)\n",
    "    residual_var = np.maximum(total_var - explained, 1e-6)\n",
    "\n",
    "    logger.info(f\"  {label} covariance: rank={r}, explained={pca.explained_variance_ratio_.sum():.3f}\")\n",
    "    return dict(\n",
    "        components=pca.components_.astype(np.float32),\n",
    "        singular_values=np.sqrt(pca.explained_variance_).astype(np.float32),\n",
    "        residual_var=residual_var.astype(np.float32),\n",
    "        rank=r,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2e4e25",
   "metadata": {},
   "source": [
    "## Batch 5: Program Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fa76e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# BATCH 5: Program Proportion Model (Enhanced)\n",
    "# ===========================================================================\n",
    "\n",
    "def batch5_program_model(stats, emb, model_dir):\n",
    "    ckpt = load_checkpoint(model_dir, \"b5_prog\")\n",
    "    if ckpt is not None:\n",
    "        return ckpt\n",
    "\n",
    "    logger.info(\"BATCH 5: Enhanced program proportion model...\")\n",
    "    training_perts = stats[\"training_perts\"]\n",
    "    prog_props = stats[\"prog_props\"]\n",
    "    gene_embeddings = emb[\"gene_embeddings\"]\n",
    "    coexpr_embeddings = emb[\"coexpr_embeddings\"]\n",
    "    gn2i = stats[\"gn2i\"]\n",
    "    embed_dim = emb[\"embed_dim\"]\n",
    "    coexpr_dim = emb[\"coexpr_dim\"]\n",
    "    control_mean = stats[\"control_mean\"]\n",
    "    control_var = stats[\"control_var\"]\n",
    "    perturbed_mean = stats[\"perturbed_mean\"]\n",
    "    response_matrix = emb[\"response_matrix\"]\n",
    "    control_pert_delta = stats[\"control_pert_delta\"]\n",
    "    sig_gene_indices = stats[\"sig_gene_indices\"]\n",
    "\n",
    "    def build_prog_features(gene_name):\n",
    "        feats = []\n",
    "        if gene_name in gn2i:\n",
    "            idx = gn2i[gene_name]\n",
    "            feats.append(gene_embeddings[idx])\n",
    "            feats.append(coexpr_embeddings[idx])\n",
    "            feats.append(np.array([\n",
    "                control_mean[idx], control_var[idx], perturbed_mean[idx],\n",
    "                response_matrix[idx].std(), response_matrix[idx].mean(),\n",
    "                control_pert_delta[idx], np.abs(response_matrix[idx]).max(), 0.0,\n",
    "            ], dtype=np.float32))\n",
    "            # Signature gene interaction features per program\n",
    "            sig_feats = []\n",
    "            for prog in PROGRAM_COLS:\n",
    "                if prog in sig_gene_indices and sig_gene_indices[prog]:\n",
    "                    sig_embed = gene_embeddings[sig_gene_indices[prog]].mean(axis=0)\n",
    "                    cos = np.dot(gene_embeddings[idx], sig_embed) / (\n",
    "                        np.linalg.norm(gene_embeddings[idx]) * np.linalg.norm(sig_embed) + 1e-8)\n",
    "                    # Also coexpression similarity\n",
    "                    sig_coexpr = coexpr_embeddings[sig_gene_indices[prog]].mean(axis=0)\n",
    "                    cos2 = np.dot(coexpr_embeddings[idx], sig_coexpr) / (\n",
    "                        np.linalg.norm(coexpr_embeddings[idx]) * np.linalg.norm(sig_coexpr) + 1e-8)\n",
    "                    sig_feats.extend([cos, cos2])\n",
    "                else:\n",
    "                    sig_feats.extend([0.0, 0.0])\n",
    "            feats.append(np.array(sig_feats, dtype=np.float32))\n",
    "        else:\n",
    "            feats.append(np.zeros(embed_dim, dtype=np.float32))\n",
    "            feats.append(np.zeros(coexpr_dim, dtype=np.float32))\n",
    "            feats.append(np.zeros(8, dtype=np.float32))\n",
    "            feats.append(np.zeros(len(PROGRAM_COLS) * 2, dtype=np.float32))\n",
    "        return np.concatenate(feats)\n",
    "\n",
    "    X, Y = [], []\n",
    "    for p in training_perts:\n",
    "        if p in prog_props:\n",
    "            X.append(build_prog_features(p))\n",
    "            Y.append([prog_props[p].get(c, 0.25) for c in PROGRAM_COLS])\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    prog_scaler = StandardScaler()\n",
    "    X_sc = prog_scaler.fit_transform(X)\n",
    "\n",
    "    # Per-program Ridge (separate model per program for better tuning)\n",
    "    prog_models = {}\n",
    "    for i, col in enumerate(PROGRAM_COLS):\n",
    "        ridge_p = RidgeCV(alphas=[0.1, 1, 10, 100, 1000, 10000], cv=None, fit_intercept=True)\n",
    "        ridge_p.fit(X_sc, Y[:, i])\n",
    "        prog_models[col] = ridge_p\n",
    "        pred = ridge_p.predict(X_sc)\n",
    "        mae = np.abs(pred - Y[:, i]).mean()\n",
    "        logger.info(f\"  {col} Ridge alpha={ridge_p.alpha_}, train MAE={mae:.4f}\")\n",
    "\n",
    "    # Also train KNN for programs\n",
    "    prog_knn = NearestNeighbors(n_neighbors=min(K_NEIGHBORS, len(training_perts) - 1), metric=\"cosine\")\n",
    "    prog_knn.fit(X_sc)\n",
    "\n",
    "    # LOO to find optimal ridge/knn blend for programs\n",
    "    best_prog_blend = 0.5\n",
    "    best_prog_l1 = float(\"inf\")\n",
    "\n",
    "    # Precompute LOO predictions\n",
    "    # 5-fold CV predictions for program Ridge\n",
    "    from sklearn.model_selection import KFold\n",
    "    kf_prog = KFold(n_splits=5, shuffle=True, random_state=RANDOM_SEED)\n",
    "    Y_ridge_prog = np.zeros_like(Y)\n",
    "    for i, col in enumerate(PROGRAM_COLS):\n",
    "        alpha_p = prog_models[col].alpha_ if hasattr(prog_models[col], 'alpha_') else 100.0\n",
    "        for fold_train, fold_test in kf_prog.split(X_sc):\n",
    "            r = Ridge(alpha=alpha_p, fit_intercept=True)\n",
    "            r.fit(X_sc[fold_train], Y[fold_train, i])\n",
    "            Y_ridge_prog[fold_test, i] = r.predict(X_sc[fold_test])\n",
    "    logger.info(\"  Computed 5-fold CV for program Ridge\")\n",
    "\n",
    "    Y_knn_prog = np.zeros_like(Y)\n",
    "    for i in range(len(training_perts)):\n",
    "        dists, idxs = prog_knn.kneighbors(X_sc[i:i+1], n_neighbors=min(K_NEIGHBORS+1, len(training_perts)))\n",
    "        mask = idxs[0] != i\n",
    "        nn_idxs = idxs[0][mask][:K_NEIGHBORS]\n",
    "        nn_dists = dists[0][mask][:K_NEIGHBORS]\n",
    "        if len(nn_idxs) > 0:\n",
    "            w = 1.0 / (nn_dists + 1e-8)\n",
    "            w /= w.sum()\n",
    "            Y_knn_prog[i] = sum(ww * Y[idx] for ww, idx in zip(w, nn_idxs))\n",
    "\n",
    "    for blend in np.arange(0, 1.025, 0.05):\n",
    "        Y_blend = blend * Y_ridge_prog + (1 - blend) * Y_knn_prog\n",
    "        Y_blend = np.clip(Y_blend, 0, 1)\n",
    "        # Normalize rows\n",
    "        rs = Y_blend.sum(axis=1, keepdims=True)\n",
    "        rs[rs == 0] = 1\n",
    "        Y_blend = Y_blend / rs\n",
    "        # Compute weighted L1 matching competition metric\n",
    "        l1_r = np.abs(Y_blend - Y).sum(axis=1).mean()  # all 4 programs: pre_adipo, adipo, lipo, other\n",
    "        lipo_adipo_pred = Y_blend[:, 2] / np.maximum(Y_blend[:, 1], 1e-8)\n",
    "        lipo_adipo_true = Y[:, 2] / np.maximum(Y[:, 1], 1e-8)\n",
    "        l1_la = np.abs(lipo_adipo_pred - lipo_adipo_true).mean()\n",
    "        score = 0.75 * l1_r + 0.25 * l1_la\n",
    "        if score < best_prog_l1:\n",
    "            best_prog_l1 = score\n",
    "            best_prog_blend = blend\n",
    "\n",
    "    logger.info(f\"  Best program blend: ridge={best_prog_blend:.2f}, LOO L1={best_prog_l1:.4f}\")\n",
    "\n",
    "    result = dict(\n",
    "        prog_models=prog_models, prog_scaler=prog_scaler, prog_knn=prog_knn,\n",
    "        best_prog_blend=best_prog_blend,\n",
    "        Y_train_prog=Y,\n",
    "    )\n",
    "    save_checkpoint(model_dir, \"b5_prog\", result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0796a25c",
   "metadata": {},
   "source": [
    "## Train Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d8f936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# TRAIN\n",
    "# ===========================================================================\n",
    "\n",
    "def train(data_directory_path, model_directory_path):\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"TRAIN v3: Obesity ML Competition - Crunch 1\")\n",
    "    logger.info(\"=\" * 60)\n",
    "    os.makedirs(model_directory_path, exist_ok=True)\n",
    "\n",
    "    done = load_checkpoint(model_directory_path, \"done\")\n",
    "    if done is not None:\n",
    "        logger.info(\"Training already complete.\")\n",
    "        return\n",
    "\n",
    "    h5ad = os.path.join(data_directory_path, \"obesity_challenge_1.h5ad\")\n",
    "    logger.info(f\"Loading: {h5ad}\")\n",
    "    adata = ad.read_h5ad(h5ad)\n",
    "    logger.info(f\"  Shape: {adata.shape}\")\n",
    "\n",
    "    stats = batch1_core_stats(adata, model_directory_path, data_directory_path)\n",
    "\n",
    "    # If b1_stats was loaded from checkpoint, pert_cells/program_cells are missing.\n",
    "    # Recompute them from adata if batch4 hasn't been cached yet.\n",
    "    if \"pert_cells\" not in stats:\n",
    "        b4_ckpt = load_checkpoint(model_directory_path, \"b4_cov\")\n",
    "        if b4_ckpt is None:\n",
    "            logger.info(\"  Recomputing cell pools for covariance (not in checkpoint)...\")\n",
    "            from scipy import sparse as sp\n",
    "            X = adata.X.toarray() if sp.issparse(adata.X) else np.asarray(adata.X)\n",
    "            X = X.astype(np.float32)\n",
    "            obs = adata.obs\n",
    "            gene_col = obs[\"gene\"].values\n",
    "            pert_cells = {}\n",
    "            for p in stats[\"training_perts\"]:\n",
    "                m = gene_col == p\n",
    "                if m.sum() > 0:\n",
    "                    pert_cells[p] = X[m][:200].copy()\n",
    "            program_cells = {}\n",
    "            rng = np.random.RandomState(RANDOM_SEED)\n",
    "            for prog in PROGRAM_COLS:\n",
    "                if prog in obs.columns:\n",
    "                    pmask = obs[prog].astype(float).values > 0.5\n",
    "                    pmask_pert = pmask & (gene_col != \"NC\")\n",
    "                    if pmask_pert.sum() > 100:\n",
    "                        max_c = min(pmask_pert.sum(), 3000)\n",
    "                        idx = rng.choice(pmask_pert.sum(), max_c, replace=False)\n",
    "                        program_cells[prog] = X[pmask_pert][idx].copy()\n",
    "            stats[\"pert_cells\"] = pert_cells\n",
    "            stats[\"program_cells\"] = program_cells\n",
    "            del X\n",
    "\n",
    "    emb = batch2_gene_embeddings(adata, stats, model_directory_path)\n",
    "    ridge = batch3_ridge_ensemble(stats, emb, model_directory_path)\n",
    "    cov = batch4_covariance(stats, model_directory_path)\n",
    "    prog = batch5_program_model(stats, emb, model_directory_path)\n",
    "\n",
    "    save_checkpoint(model_directory_path, \"done\", {\"v\": 3})\n",
    "    logger.info(\"TRAINING COMPLETE.\")\n",
    "\n",
    "    # Explicit memory cleanup: free adata and large intermediates\n",
    "    # before infer() is called in the same process\n",
    "    del adata, stats, emb, ridge, cov, prog\n",
    "    import gc; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defb4476",
   "metadata": {},
   "source": [
    "## Infer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0b736f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===========================================================================\n",
    "# INFER\n",
    "# ===========================================================================\n",
    "\n",
    "def infer(\n",
    "    data_directory_path,\n",
    "    prediction_directory_path,\n",
    "    prediction_h5ad_file_path,\n",
    "    program_proportion_csv_file_path,\n",
    "    model_directory_path,\n",
    "    predict_perturbations,\n",
    "    genes_to_predict,\n",
    "):\n",
    "    logger.info(\"=\" * 60)\n",
    "    logger.info(\"INFER v3: Obesity ML Competition - Crunch 1\")\n",
    "    logger.info(f\"  {len(predict_perturbations)} perturbations x {len(genes_to_predict)} genes\")\n",
    "    logger.info(\"=\" * 60)\n",
    "\n",
    "    rng = np.random.RandomState(RANDOM_SEED)\n",
    "\n",
    "    stats = load_checkpoint(model_directory_path, \"b1_stats\")\n",
    "    emb = load_checkpoint(model_directory_path, \"b2_embed\")\n",
    "    ridge_data = load_checkpoint(model_directory_path, \"b3_ridge\")\n",
    "    cov_data = load_checkpoint(model_directory_path, \"b4_cov\")\n",
    "    prog_data = load_checkpoint(model_directory_path, \"b5_prog\")\n",
    "\n",
    "    if stats is None:\n",
    "        raise RuntimeError(\"Checkpoints not found. Run train() first.\")\n",
    "\n",
    "    # Unpack\n",
    "    gene_names = stats[\"gene_names\"]\n",
    "    gn2i = stats[\"gn2i\"]\n",
    "    n_genes_full = stats[\"n_genes\"]\n",
    "    perturbed_mean = stats[\"perturbed_mean\"]\n",
    "    control_mean = stats[\"control_mean\"]\n",
    "    control_var = stats[\"control_var\"]\n",
    "    control_pert_delta = stats[\"control_pert_delta\"]\n",
    "    pert_means = stats[\"pert_means\"]\n",
    "    pert_vars = stats[\"pert_vars\"]\n",
    "    global_var = stats[\"global_var\"]\n",
    "    avg_props = stats[\"avg_props\"]\n",
    "    prog_props = stats[\"prog_props\"]\n",
    "    training_perts = stats[\"training_perts\"]\n",
    "    program_profiles = stats[\"program_profiles\"]\n",
    "    sig_gene_indices = stats[\"sig_gene_indices\"]\n",
    "\n",
    "    gene_embeddings = emb[\"gene_embeddings\"]\n",
    "    coexpr_embeddings = emb[\"coexpr_embeddings\"]\n",
    "    response_matrix = emb[\"response_matrix\"]\n",
    "    embed_dim = emb[\"embed_dim\"]\n",
    "    coexpr_dim = emb[\"coexpr_dim\"]\n",
    "\n",
    "    ridge_all = ridge_data[\"ridge_all\"]\n",
    "    ridge_reg = ridge_data[\"ridge_reg\"]\n",
    "    ridge_scaler = ridge_data[\"scaler\"]\n",
    "    best_w = ridge_data[\"best_weights\"]\n",
    "    shrinkage = ridge_data.get(\"shrinkage\", 1.0)\n",
    "    noise_scale = ridge_data[\"noise_scale\"]\n",
    "    prog_delta_blend = ridge_data.get(\"prog_delta_blend\", 0.6)\n",
    "\n",
    "    knn = emb[\"knn\"]\n",
    "    feat_scaler = emb[\"feat_scaler\"]\n",
    "    X_train_scaled = emb[\"X_train_scaled\"]\n",
    "    Y_train = emb[\"Y_train\"]\n",
    "\n",
    "    global_cov = cov_data[\"global_cov\"]\n",
    "    program_covs = cov_data[\"program_covs\"]\n",
    "\n",
    "    prog_models = prog_data[\"prog_models\"] if prog_data else None\n",
    "    prog_scaler = prog_data[\"prog_scaler\"] if prog_data else None\n",
    "    prog_knn = prog_data[\"prog_knn\"] if prog_data else None\n",
    "    best_prog_blend = prog_data[\"best_prog_blend\"] if prog_data else 0.5\n",
    "    Y_train_prog = prog_data[\"Y_train_prog\"] if prog_data else None\n",
    "\n",
    "    # Gene index mapping\n",
    "    pred_gene_idx = np.array([gn2i.get(g, -1) for g in genes_to_predict])\n",
    "    n_perts = len(predict_perturbations)\n",
    "    n_genes_out = len(genes_to_predict)\n",
    "\n",
    "    def build_gene_features(gene_name):\n",
    "        feats = []\n",
    "        if gene_name in gn2i:\n",
    "            idx = gn2i[gene_name]\n",
    "            feats.append(gene_embeddings[idx])\n",
    "            feats.append(coexpr_embeddings[idx])\n",
    "            feats.append(np.array([\n",
    "                control_mean[idx], control_var[idx], perturbed_mean[idx],\n",
    "                response_matrix[idx].std(), response_matrix[idx].mean(),\n",
    "                control_pert_delta[idx], np.abs(response_matrix[idx]).max(), 0.0,\n",
    "            ], dtype=np.float32))\n",
    "        else:\n",
    "            feats.append(np.zeros(embed_dim, dtype=np.float32))\n",
    "            feats.append(np.zeros(coexpr_dim, dtype=np.float32))\n",
    "            feats.append(np.zeros(8, dtype=np.float32))\n",
    "        return np.concatenate(feats)\n",
    "\n",
    "    def build_prog_features(gene_name):\n",
    "        base = build_gene_features(gene_name)\n",
    "        sig_feats = []\n",
    "        if gene_name in gn2i:\n",
    "            idx = gn2i[gene_name]\n",
    "            for prog in PROGRAM_COLS:\n",
    "                if prog in sig_gene_indices and sig_gene_indices[prog]:\n",
    "                    se = gene_embeddings[sig_gene_indices[prog]].mean(axis=0)\n",
    "                    cos1 = np.dot(gene_embeddings[idx], se) / (np.linalg.norm(gene_embeddings[idx]) * np.linalg.norm(se) + 1e-8)\n",
    "                    sc = coexpr_embeddings[sig_gene_indices[prog]].mean(axis=0)\n",
    "                    cos2 = np.dot(coexpr_embeddings[idx], sc) / (np.linalg.norm(coexpr_embeddings[idx]) * np.linalg.norm(sc) + 1e-8)\n",
    "                    sig_feats.extend([cos1, cos2])\n",
    "                else:\n",
    "                    sig_feats.extend([0.0, 0.0])\n",
    "        else:\n",
    "            sig_feats = [0.0] * (len(PROGRAM_COLS) * 2)\n",
    "        return np.concatenate([base, sig_feats])\n",
    "\n",
    "    def _extract_output(full_vec):\n",
    "        \"\"\"Extract output gene values from full gene vector.\"\"\"\n",
    "        out = np.zeros(n_genes_out, dtype=np.float32)\n",
    "        for j, gi in enumerate(pred_gene_idx):\n",
    "            if gi >= 0:\n",
    "                out[j] = full_vec[gi]\n",
    "        return out\n",
    "\n",
    "    def _extract_cov_output(cov_dict):\n",
    "        \"\"\"Extract covariance components for output genes.\"\"\"\n",
    "        comp = cov_dict[\"components\"]\n",
    "        sv = cov_dict[\"singular_values\"]\n",
    "        rv = cov_dict[\"residual_var\"]\n",
    "        r = cov_dict[\"rank\"]\n",
    "        comp_out = np.zeros((r, n_genes_out), dtype=np.float32)\n",
    "        rv_out = np.zeros(n_genes_out, dtype=np.float32)\n",
    "        for j, gi in enumerate(pred_gene_idx):\n",
    "            if gi >= 0:\n",
    "                comp_out[:, j] = comp[:r, gi]\n",
    "                rv_out[j] = rv[gi]\n",
    "        return comp_out, sv[:r], rv_out, r\n",
    "\n",
    "    def sample_cells(mean_full, n_cells, prog_weights, pert_noise_override=None):\n",
    "        \"\"\"Generate cells using per-program covariance and mixture model.\"\"\"\n",
    "        ns = pert_noise_override if pert_noise_override is not None else noise_scale\n",
    "        cells = np.zeros((n_cells, n_genes_out), dtype=np.float32)\n",
    "\n",
    "        if prog_weights and program_profiles:\n",
    "            progs = list(prog_weights.keys())\n",
    "            weights = np.array([max(prog_weights.get(p, 0), 0) for p in progs])\n",
    "            ws = weights.sum()\n",
    "            weights = weights / ws if ws > 0 else np.ones(len(progs)) / len(progs)\n",
    "            n_per_prog = rng.multinomial(n_cells, weights)\n",
    "\n",
    "            cell_idx = 0\n",
    "            for prog, n_prog in zip(progs, n_per_prog):\n",
    "                if n_prog == 0:\n",
    "                    continue\n",
    "\n",
    "                # Program-specific mean\n",
    "                if prog in program_profiles:\n",
    "                    delta = mean_full - perturbed_mean\n",
    "                    adj_mean = program_profiles[prog] + delta * prog_delta_blend\n",
    "                else:\n",
    "                    adj_mean = mean_full\n",
    "                mean_out = _extract_output(adj_mean)\n",
    "\n",
    "                # Per-program covariance\n",
    "                cov = program_covs.get(prog, global_cov)\n",
    "                comp_out, sv, rv_out, r = _extract_cov_output(cov)\n",
    "\n",
    "                # Sample: structured + diagonal noise\n",
    "                z = rng.randn(n_prog, r).astype(np.float32)\n",
    "                structured = z @ (sv.reshape(-1, 1) * comp_out) * ns\n",
    "                diagonal = rng.randn(n_prog, n_genes_out).astype(np.float32) * np.sqrt(rv_out) * ns\n",
    "\n",
    "                cells[cell_idx:cell_idx + n_prog] = mean_out + structured + diagonal\n",
    "                cell_idx += n_prog\n",
    "        else:\n",
    "            mean_out = _extract_output(mean_full)\n",
    "            comp_out, sv, rv_out, r = _extract_cov_output(global_cov)\n",
    "            z = rng.randn(n_cells, r).astype(np.float32)\n",
    "            structured = z @ (sv.reshape(-1, 1) * comp_out) * ns\n",
    "            diagonal = rng.randn(n_cells, n_genes_out).astype(np.float32) * np.sqrt(rv_out) * ns\n",
    "            cells = mean_out + structured + diagonal\n",
    "\n",
    "        # Softplus for near-zero values: avoids point mass at 0 that distorts MMD\n",
    "        # For values > 5, effectively identity; for values near 0, smooth curve to 0\n",
    "        neg_mask = cells < 0\n",
    "        cells[neg_mask] = np.log1p(np.exp(cells[neg_mask] * 3)) / 3  # steep softplus\n",
    "        return cells\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # MAIN PREDICTION LOOP\n",
    "    # -----------------------------------------------------------------------\n",
    "    # Pre-allocate the full output matrix. Each perturbation writes its\n",
    "    # 100 cells directly into the pre-allocated block, avoiding a separate\n",
    "    # all_predictions list (saves 11.7GB of duplicated memory).\n",
    "    logger.info(f\"  Pre-allocating output: ({n_perts * CELLS_PER_PERTURBATION} x {n_genes_out})...\")\n",
    "    mat = np.zeros((n_perts * CELLS_PER_PERTURBATION, n_genes_out), dtype=np.float32)\n",
    "    all_obs_genes = []\n",
    "    program_rows = []\n",
    "\n",
    "    for pi, pert in enumerate(predict_perturbations):\n",
    "        if pi % 500 == 0:\n",
    "            logger.info(f\"  [{pi}/{n_perts}] {pert}\")\n",
    "\n",
    "        # === PREDICT MEAN EXPRESSION ===\n",
    "        if pert in pert_means:\n",
    "            pred_mean = pert_means[pert].copy()\n",
    "        else:\n",
    "            feat = build_gene_features(pert).reshape(1, -1)\n",
    "            feat_sc = ridge_scaler.transform(feat)\n",
    "\n",
    "            # Strategy 1: Ridge all-genes\n",
    "            d1 = ridge_all.predict(feat_sc)[0]\n",
    "            # Strategy 2: Ridge regularized\n",
    "            d2 = ridge_reg.predict(feat_sc)[0]\n",
    "            # Strategy 3: KNN\n",
    "            feat_knn = feat_scaler.transform(feat)\n",
    "            dists, idxs = knn.kneighbors(feat_knn)\n",
    "            w = 1.0 / (dists[0] + 1e-8)\n",
    "            w /= w.sum()\n",
    "            d3 = sum(ww * Y_train[idx] for ww, idx in zip(w, idxs[0]))\n",
    "\n",
    "            # Ensemble + shrinkage toward perturbed mean\n",
    "            blended = best_w[0] * d1 + best_w[1] * d2 + best_w[2] * d3\n",
    "            blended *= shrinkage\n",
    "            pred_mean = perturbed_mean + blended\n",
    "\n",
    "        # === PREDICT PROGRAM PROPORTIONS ===\n",
    "        if pert in prog_props:\n",
    "            props = dict(prog_props[pert])\n",
    "            # Normalize to sum to 1.0 (safety for edge cases)\n",
    "            total = sum(props.get(c, 0) for c in PROGRAM_COLS)\n",
    "            if total > 0:\n",
    "                for c in PROGRAM_COLS:\n",
    "                    props[c] = props.get(c, 0) / total\n",
    "            else:\n",
    "                props = dict(avg_props)\n",
    "        elif prog_models is not None:\n",
    "            feat = build_prog_features(pert).reshape(1, -1)\n",
    "            feat_s = prog_scaler.transform(feat)\n",
    "\n",
    "            # Ridge predictions per program\n",
    "            ridge_pred = np.array([prog_models[col].predict(feat_s)[0] for col in PROGRAM_COLS])\n",
    "            # KNN prediction\n",
    "            dists, idxs = prog_knn.kneighbors(feat_s)\n",
    "            w = 1.0 / (dists[0] + 1e-8)\n",
    "            w /= w.sum()\n",
    "            knn_pred = sum(ww * Y_train_prog[idx] for ww, idx in zip(w, idxs[0]))\n",
    "\n",
    "            pred_p = best_prog_blend * ridge_pred + (1 - best_prog_blend) * knn_pred\n",
    "            pred_p = np.clip(pred_p, 0, 1)\n",
    "            total = pred_p.sum()\n",
    "            pred_p = pred_p / total if total > 0 else np.array([avg_props[c] for c in PROGRAM_COLS])\n",
    "            props = {c: float(pred_p[i]) for i, c in enumerate(PROGRAM_COLS)}\n",
    "        else:\n",
    "            props = dict(avg_props)\n",
    "\n",
    "        # === GENERATE CELLS ===\n",
    "        # For unseen perturbations, use KNN neighbors' variance for adaptive noise\n",
    "        pert_noise_scale = noise_scale\n",
    "        if pert not in pert_means and pert_vars:\n",
    "            # Get KNN neighbor indices from the prediction step\n",
    "            feat = build_gene_features(pert).reshape(1, -1)\n",
    "            feat_knn = feat_scaler.transform(feat)\n",
    "            try:\n",
    "                _, nn_idxs = knn.kneighbors(feat_knn)\n",
    "                nn_perts = [training_perts[j] for j in nn_idxs[0]]\n",
    "                nn_vars = [pert_vars[p].mean() for p in nn_perts if p in pert_vars]\n",
    "                if nn_vars:\n",
    "                    nn_mean_var = np.mean(nn_vars)\n",
    "                    global_mean_var = np.mean([pert_vars[p].mean() for p in training_perts if p in pert_vars])\n",
    "                    # Scale noise proportionally: if neighbors are high-variance, increase noise\n",
    "                    var_ratio = np.sqrt(nn_mean_var / (global_mean_var + 1e-10))\n",
    "                    pert_noise_scale = noise_scale * np.clip(var_ratio, 0.5, 2.0)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "        cells = sample_cells(pred_mean, CELLS_PER_PERTURBATION, prog_weights=props,\n",
    "                           pert_noise_override=pert_noise_scale)\n",
    "\n",
    "        start = pi * CELLS_PER_PERTURBATION\n",
    "        mat[start:start + CELLS_PER_PERTURBATION] = cells\n",
    "        del cells\n",
    "        all_obs_genes.extend([pert] * CELLS_PER_PERTURBATION)\n",
    "\n",
    "        adipo_v = max(props.get(\"adipo\", 1e-8), 1e-8)\n",
    "        program_rows.append({\n",
    "            \"gene\": pert,\n",
    "            \"pre_adipo\": props.get(\"pre_adipo\", avg_props[\"pre_adipo\"]),\n",
    "            \"adipo\": props.get(\"adipo\", avg_props[\"adipo\"]),\n",
    "            \"lipo\": props.get(\"lipo\", avg_props[\"lipo\"]),\n",
    "            \"other\": props.get(\"other\", avg_props[\"other\"]),\n",
    "            \"lipo_adipo\": props.get(\"lipo\", avg_props[\"lipo\"]) / adipo_v,\n",
    "        })\n",
    "\n",
    "    # -----------------------------------------------------------------------\n",
    "    # WRITE OUTPUTS\n",
    "    # -----------------------------------------------------------------------\n",
    "    logger.info(\"Writing prediction.h5ad...\")\n",
    "    n_total = n_perts * CELLS_PER_PERTURBATION\n",
    "    logger.info(f\"  Dense matrix: {mat.shape}, ~{mat.nbytes / 1e9:.1f}GB\")\n",
    "\n",
    "    pred_ad = ad.AnnData(\n",
    "        X=mat,\n",
    "        obs=pd.DataFrame({\"gene\": all_obs_genes}),\n",
    "        var=pd.DataFrame(index=genes_to_predict),\n",
    "    )\n",
    "    del mat\n",
    "    import gc; gc.collect()\n",
    "    pred_ad.write_h5ad(prediction_h5ad_file_path)\n",
    "\n",
    "    logger.info(f\"  Saved: {prediction_h5ad_file_path}\")\n",
    "\n",
    "    prog_df = pd.DataFrame(program_rows)\n",
    "    prog_df = prog_df[[\"gene\", \"pre_adipo\", \"adipo\", \"lipo\", \"other\", \"lipo_adipo\"]]\n",
    "    prog_df.to_csv(program_proportion_csv_file_path, index=False)\n",
    "    logger.info(f\"  Saved: {program_proportion_csv_file_path}\")\n",
    "    logger.info(\"INFERENCE COMPLETE.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
